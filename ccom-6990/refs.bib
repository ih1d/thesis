@inproceedings{tfmodel,
  title={A computational model for TensorFlow: an introduction},
  author={Abadi, Mart{\'\i}n and Isard, Michael and Murray, Derek G},
  booktitle={Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={1--7},
  year={2017},
  annotate={This paper discusses how TensorFlow works under the hood, by explaining a very limited version of TensorFlow's dataflow computational model.
  The limited version contains variables, tensors, and read and write operations on variables. The authors explain the semantics of these operations, and how the dataflow graph behaves.}
}

@inproceedings{omlpl,
  title={On machine learning and programming languages},
  author={Innes, Mike and Karpinski, Stefan and Shah, Viral and Barber, David and Saito Stenetorp, PLEPS and Besard, Tim and Bradbury, James and Churavy, Valentin and Danisch, Simon and Edelman, Alan and others},
  year={2018},
  organization={Association for Computing Machinery (ACM)},
  annotate={This paper argues that a new language for machine learning is needed. There are various arguments: libraries like TensorFlow 
  are already languages in themselves, current languages work as meta-languages, and a new language could improve certain features.
  This source is helpful because it lays the basis, informally, as to what is needed for a programming language based on machine learning.
  Its goal is to explain what features such a language has, what similarities it has with other languages, and how does it differ from conventional
  programming languages. It changes the approach on how to implement a language or what the target of my language should be.}
}

@inproceedings{imptograph,
author = {V\'{e}lez, Tatiana Castro and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Raja, Anita},
title = {Challenges in migrating imperative deep learning programs to graph execution: an empirical study},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528455},
doi = {10.1145/3524842.3528455},
abstract = {Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. While hybrid approaches aim for the "best of both worlds," the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges---and resultant bugs---involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation---the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {469â€“481},
numpages = {13},
keywords = {deep learning, empirical studies, graph-based execution, hybrid programming paradigms, imperative programs, software evolution},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22},
annotate={This paper surveys and reviews more than 19 million lines of code to try and identify common bugs 
on code that migrates imperative style to graph execution programs. This source helps by reinforcing my idea since it presents a large amount
of bugs and performance issues. The source can be used as a metric to show how a language can improve the code quality or performance.}
}

@book{lambda,
  title={A Lambda Calculus Satellite},
  author={Barendregt, Hendrik Pieter and Manzonetto, Giulio},
  year={2022},
  publisher={College publications}
}

@misc{denotations,
  title={Denotational Semantics: The Scott-Strachey Approach},
  author={Stoy, Joseph E},
  year={1977},
  publisher={MIT Press Cambridge, Mass.}
}